\chapter{Implementation}
\label{c:implementation}

In this chapter we will discuss a practical implementation, \emph{spexs2}, for pattern discovery in sequences. We only discuss parts that we consider non-trivial or interesting and could be useful in implementing other algorithms.

Information about the full source code is in the Appendix \ref{add:spexs2}.

\section{Language}

The language choice in a project is very subjective. Usually the language chosen is either the language that the author feels most comfortable with or widely used language that has many libraries. The decision to use Go was based on several informal sources\cite{HammerPrinciple,LanguageShootout,LoopRecognition,go,clojure,julia} and the following discussion should be seen as opinions rather than facts.

This project should be a reference implementation and should be as readable as possible; languages such as C++, Java are probably not the best choices due to their complexity. The project deals with a lot of generic code; this means Clojure, Haskell and OCaml would be useful, but they would require a lot of effort if someone wishes to contribute and doesn't know the language. Languages such as Matlab, Octave or R would be ideal due to their ease of use, but their speed or memory performance is worse than C.

There are newer languages Go, D and Julia that suit the problem better. D is an high performance language that was designed as an replacement for C++, but still has a steep learning curve. Julia is a high-performance dynamic language designed for technical computing, but at the time of starting the project it had significant language runtime bugs. Go is a systems language designed to be simple, but it is worse in performance than D and Julia.

Go seemed to be the best choice due its simplicity, stability and concurrency support. The performance characteristics also seemed good enough. Language simplicity has several benefits: ease of learning, readability and simplicity. Ease of learning means people can more easily contribute to the project. Readability means that it serves a better reference implementation. Simplicity means that most of the language semantics and structures can be directly translated to other languages if there is a need for better performance.

In summary, Go is a nice compromise between Python and C. It adds additional rigidity and concurrency primitives and is a simple language.

\section{Architecture}

The main criteria for designing program have been described in D. Parnas
paper "On the Criteria To Be Used in Decomposing Systems into Modules"\cite{Parnas72}. It suggests decomposing into isolated units and parts that are likely to change together.

We chose the following module decomposition for the application:

\begin{small}
\begin{description}
    \itemsep-0.5em
    \item[Configuration] structure for holding the configuration data
    \item[Setup] based on the configuration initializes data-structures and functions for the algorithm
    \item[Reader] reads in the data from files
    \item[Database] a collection of datasets
    \item[Algorithm] the SPEXS2 algorithm
    \item[Printer] prints the result queries
\end{description}
\end{small}

The program starts by interpreting flags, then it marshals configuration file onto an internal data structure, this configuration structure as an input to the setup module. Setup module initializes (as defined by configuration) a reader, a printer and also prepares structures for algorithm. Then the reader reads input to the database. Then the algorithm is activated and finally it is printed out with Printer.

This structure is universal for algorithm implementations and allows easily to add more configuration options, different input formats and different output formats. By changing the configuration, reader and printer we could make it a web service instead of running it on a command line.

\section{Configuration}

One problem with flexible algorithms is that there are a lot of ways to be run. This can lead to having tens or hundreds of command-line flags for the application.

To avoid this problem we decided to use a \emph{json} file for the program configuration. This format is widely known and well structured. For example a configuration file for pattern discovery in protein sequences:

\begin{file}
{
    "Dataset": {
        "fore" : { "File" : "$inp$" },
        "back" : { "File" : "$ref$" }
    },
    "Reader" : {
        "Method" : "Delimited"
    },
    "Extension": {
        "Method": "Group",
        "Groups" : {
            "." : { "elements" : "ACDEFGHIKLMNPRQSTVWY"}
        },
        "Extendable": {
            "PatGroups()" : {"max" : 3},
            "PatLength()" : {"max" : 6},
            "Matches(fore)" : {"min" : 20},
            "NoStartingGroup()" : {}
        },
        ...
    },
    "Output": {
        "SortBy": ["-Hyper(fore, back)"],
        "Count": 100
    },
    "Printer" : {
        "Method" : "Formatted",
        "Format": "Pat?()\t...\tHyper(fore,back)\n"
    }
}
\end{file}

In hindsight \emph{json} for configuration may not be the best option due to rigidity. Users can often forget to add or remove leading commas or forget to add quotes. This suggests that formats such as \emph{rson}\insertref{rson} or \emph{yaml}\insertref{yaml} would be better choices.

Other problem with configuration files is that they are harder to modify than command-line flags. By mixing command-line flags and configuration files we can get a solution that works better in practice than either of them independently.

One easy way to implement is to add custom syntax into the configuration file:

\begin{file}
"Datasets" : {
    "fore" : { "File" : "$argument:default$"
    ...
\end{file}

Now some command-flags can be interpreted as replacements into the configuration file. Using \cmdline{spexs2 --conf conf.json argument=other} would transform the configuration file into:

\begin{file}
"Datasets" : {
    "fore" : { "File" : "other"
    ...
\end{file}

If no such parameter is given then the default value "main" can be used.

Configuration files are also problematic for running the first time. As a user you need to find a configuration file that suits your needs, then modify it and finally run it. To remedy this problem the application can embed sample configuration files so called "profiles" that can be used directly from the command line. This means you can directly use \cmdline{spexs2 -p=protein input=some.data min-p=1.0}.

\section{Alphabet and Database}

\emph{spexs2} was designed also to work with texts and words. To support such large alphabets we first needed to map each token to an identifier (an integer) and convert the sequences to a sequence of these identifiers. Unfortunately this is problematic for large datasets since this can take a lot of time as compared to just copying the file to memory.

There are analyses that may require many datasets. For example instead of comparing two datasets at a time you may want to compare pair-wise multiple datasets at the same time. Supporting multiple datasets in the code is rather trivial, but exposing this to the user is more interesting.

To support multiple datasets we first added a name for each dataset in the configuration:

\begin{file}
"Datasets" : {
    "A" : { "File" : "$A$" },
    "B" : { "File" : "$B$" },
    "C" : { "File" : "$C$" },
    ...
\end{file}

We also use the command-line argument syntax to make it easier to use. Now when we are defining filters we need to specify how exactly they are calculated. For example we can not just say ratio of occurrences in dataset since that would be ambigous. We used syntax similar to function calls in the configuration:

\begin{file}
...
"Extension": {
    "Outputtable": {
        "OccurencesRatio(A, B)" : {"min" : 2},
        ...
\end{file}

This allows to clearly see that the occurrences ratio between datasets A and B must be at least 2. Obviously we can also define features that take more arguments.

\section{Pools}

The input pool can direct the flow of extending process, which in turn can affect performance and memory. The performance doesn't get affected as much because the algorithm is exhaustive and hence every query gets examined, unless it is terminated early or the filters tuned during runtime.

There are several ways run the extension process: breadth first, depth first, most frequent first, least frequent first and others. Breadth first and depth first can easily be achieved by a queue and a stack respectively. We can use priority queues to implement other ways of extending.

The major concern was running on large datasets which meant that the memory consumption important. Although the most frequent first can arrive to the interesting results faster it can also use a lot of memory due to unextended less frequent nodes.

The least frequent approach would minimize the memory use since the least frequent would be most likely to be discarded by filters. This requires the use of a priority queue. The depth first approach would also use little memory but can use a stack.

Since we need concurrent access to the pool it requires very fast adding and removing operations. The depth-first ordering is more suitable since stack operations are faster than priority queues.

For the output queue a priority queue is the obvious choice since we usually only want a limited number of the most interesting results.

\section{Query and Location set}

The structure of query required some special consideration. The proposed solution in "Pattern Discovery from Biosequences" was to use a trie for remembering each pattern and then use optimized set for storing locations inside the database. Since the algorithm must now work concurrently using a simple trie was not an option because adding a child to the parent from multiple processes causes a race condition. 

The first approach was to flip the trie, which means instead of parent pointing to the child the child points to the parent. The original tree can be extracted by reversing the links after the algorithm has finished. This started causing problems since we are working in a garbage collected environment and each pointer adds overhead to the garbage collection. Rough estimation also suggested that memory benefit from using trie is minimal.

The second approach was just to copy the any needed content to the child and not to link them. This worked out very well.

The other problem was how to store the position list. Initially it seemed that a very tightly packed set structure is required to keep the memory usage of the program minimal. This of course would have impacted the performance. This actually turned out not to be the case since all the queries could be packed with any packing algorithm when they are stored in the pools and that had similar memory benefit with a simpler code.

One interesting memory optimization we found was a related to storing sparse positions. Because there wasn't an memory efficient set implementation for Go we needed to implement an integer set. A trivial way to implement large bitset is using a hash map, where the values are bit-arrays. We use the first bits as the key and the rest store in the bit array. We know that the occurrences of a pattern are likely to be sparse, hence it is also quite likely there are only bit arrays where only a single bit is set. In a sense, this sort of sparse data is the worst case scenario for this implementation. The solution is to swap $k$ lower bits with some $k$ higher bits. This means that the key bits will be less uniform and hence it is more likely that multiple numbers end up in the same bitarray.

For example, lets assume we have 8bit integers and we can store 3 bit numbers in the bitarray. We swap bits 2 and 1 with bits 4 and 3.

\begin{center}
    \small
    \begin{tabular}{ | r | l | l | }
    \hline
    number & binary & swapped \\
    index & 76543.210 & 76521.430 \\
    \hline
     7 & 00001.011 & 00001.011 \\
    12 & 00011.000 & 00000.110 \\
    19 & 00100.011 & 00101.001 \\
    26 & 00110.010 & 00101.100 \\
    32 & 01000.000 & 01000.000 \\
    37 & 01001.001 & 01000.011 \\
    43 & 01010.011 & 01001.101 \\
    50 & 01100.010 & 01101.000 \\
    56 & 01110.000 & 01100.100 \\
    61 & 01111.001 & 01100.111 \\
    \hline
    \end{tabular}    
\end{center}

We can see that, in the unswapped case, the data structure has the worst case scenario where each number would be stored in a separate bitarray. After swapping the bits we have made them "less uniformly distributed" and reduced the number of bit-arrays from 10 to 7.

Of course, "the best" bit-swapping positions and key/value size vary depending on the numbers being stored. When we are using large integers we can additionally make an additional layer to reduce the memory overhead. In our case, after tuning, it used about 2x less memory than the trivial implementation.

\section{Query features, interestingness and filters}

When we first described the query features we showed that filters and interestingness are a special case query features. In \emph{spexs2} the features are used to print information about the results.

Since most of the features implemented could also be used as a interestingness measure we used a simplification for the "Feature" function definition:

\begin{file}
type Feature func(q *Query) (float64, string)
\end{file}

Which means that the function returns two types, a real value and a string. In the implementation there are only few features that return arbitrary types so it was easier to convert it into a string. The only place where such features were needed is for printing. For example one of such features is the representation of the pattern.

And an implementation of a Feature constructor:

\begin{file}
// the count of matching sequences
func Matches(datasets []int) Feature {
    return func(q *Query) (float64, string) {
        matches := q.Matches()
        return countf(matches, group), ""
    }
}    
\end{file}

The argument to the function defines over which datasets the resulting function counts matches. To bind this constructor to the configuration we register it and with reflection use the function name as the name used in the configuration file.

The filters can be very similar to features in their implementation. For example a filter for pattern length is similar to the pattern length feature. Although implementing all combinations is possible we can use function composition to avoid such repetition.

By specifying a minimum or a maximum value for a feature we can turn it into a filter. One way to do it is using a lexical closure. For example:

\begin{file}
func MakeFeatureFilter(fn Feature, min, max float64) Filter {
    return func(q *Query) bool {
        v, _ := feature(q)
        return (min <= v) && (v <= max)
    }
}
\end{file}

There are some filters that cannot be defined by features so there is still a possibility to make separate filters. For example disallowing certain tokens in pattern is defined as a separate filter.

In languages which do not support such composition we could use object composition and/or function pointers.

\section{Synchronized Graph Traversal}

\emph{spexs2} can be seen as a pattern tree traversal algorithm with some extra logic. Implementing search over a tree requires synchronization such that there are only a certain number of workers and that they wouldn't die of starvation. For universality we describe the principle for graphs.

Without synchronization the parallel version looks like:

\begin{algorithm}[H]
    \caption{Graph traversal}
\begin{algorithmic}[1]
    \Ensure{All nodes in tree get processed with fn}

    \Function{visit}{tree, start, fn, examine?}
        \Let{$unvisited$}{\{ start \})}
        
        \Spawn
            \While{$unvisited$ not empty}
                \Let{$node$}{$unvisited$.take()}
                \State fn($node$)
                \For{ $child \in$ children($node$) }
                    \If{ examine?($child$) }
                        \State $unvisited$.put($child$)
                    \EndIf
                \EndFor
            \EndWhile
        \EndSpawn

        \State{wait for workers}
    \EndFunction
\end{algorithmic}
\end{algorithm}

This would not work correctly with multiple workers since there are race conditions and the workers can die early due to starvation.

The solution is to control worker startup and only terminate workers if all have finished and there are no more items in unvisited set.

\begin{algorithm}[H]
    \caption{Synchronized graph traversal}
\begin{algorithmic}[1]
    \Ensure{All nodes in $graph$ get processed with $fn$}

    \Function{Visit}{graph, start, fn, examine?}
        \Let{added}     {new semaphore(0)}
        \Let{$terminate$} {false}
        \Let{mutex}     {new mutex()}
        \Let{$workers$}   {0}
        \Let{$unvisited$} {\{ start \}}
        \State $added$.signal()
        
        \Spawn
            \While{$true$}
                \State added.wait()
                \State mutex.lock()
                \If{ $terminate$ }
                    \State added.signal()
                    \State mutex.unlock()
                    \State{\textbf{break}}
                \EndIf

                \Let{$node$}{$unvisited$.take()}
                \Let{$workers$}{$workers$ + 1}
                \State mutex.unlock()
                \Statex
                \State fn($node$)
                \Statex
                \For{ $child \in$ children($node$) }
                    \State mutex.lock()
                    \If{ examine?($child$) }
                        \State $unvisited$.put($child$)
                        \State added.signal()
                    \EndIf
                    \State mutex.unlock()
                \EndFor
                \Statex
                \State mutex.lock()
                \Let{$workers$}{$workers - 1$}
                \If{$workers = 0$ and $unvisited = \{\}$}
                    \Let{$terminate$}{true}
                    \State added.signal()
                \EndIf
                \State mutex.unlock()
            \EndWhile
        \EndSpawn

        \State{wait for workers}
    \EndFunction
\end{algorithmic}
\end{algorithm}

We use $mutex$ to protect variables and data structures. Semaphore $added$ tracks how many items are in the $unvisited$ set, if the process finally terminates it is turned into a turnstile on line 31 and 13. Variable $workers$ tracks how many workers are busy.

\section{Debugging}

Seeing how the algorithm runs is very useful to get an understanding how the algorithm works. This often can help to either improve the input configuration or debug the program itself. Often such tracing is implemented by adding debug statements.

For example:

\begin{file}
func Spexs(s *Setup) {
    for q, ok := s.In.Pop(); ok {
        trace("started extending %v", q)
        extended := s.Extend(q)
        trace("extension result %v", extended)
        for qx := range extended {
            if s.Extendable(qx) {
                trace(" > extendable %v" qx)
                s.In.Push(qx)
                if s.Outputtable(qx) {
                    trace(" > outputtable %v" qx)
                    s.Out.Push(qx)
                }
            }
        }
    }
}
\end{file}

Such statements make it harder to read the actual code, also it's hard to modify the statements for debugging or provide different ways of debugging.

We can use lexical closures to make it simpler:

\begin{file}
type Extender func(q Query) []Query

func AddDebuggingStatements(s *Setup) {
    fn := s.Extend
    s.Extend := func(q Query) []Query {
        trace("started extending %v", q)
        extended := fn(q)
        trace("extension result %v", extended)
        
        for qx := range extended {
            trace(" > %v", qx)
            trace(" > extendable %v", s.Extendable(qx))
            trace(" > outputtable %v", s.Outputtable(qx))
        }
        return extended
    }
}

func Spexs(s *Setup) {
    for q, ok := s.In.Pop(); ok {
        extended := s.Extend(q)
        for qx := range extended {
            if s.Extendable(qx) {
                s.In.Push(qx)
                if s.Outputtable(qx) {
                    s.Out.Push(qx)
                }
            }
        }
    }
}

func run(){
    S := CreateSpexsSetup()
    if debugMode {
        AddDebuggingStatements(S)
    }
    Spexs(S)
}
\end{file}

We have removed the debugging statements from the algorithm. We could define other "debug statement injectors" that provide different levels of details. This method of course has a slight performance impact due to the additional indirection. This can be extended to provide user interaction and other features.