\chapter{Approaches to Pattern Discovery}
\label{c:algorithms}

Pattern discovery algorithms can be organized in different ways\cite{SurveyDNAMotif, SurveyMotifDiscovery, CombinatorialSubtle, Hausler05}. In this chapter we give an overview of the different ideas; for thorough descriptions we suggest "Motif Discovery on Promotor Sequences" by M. Häußler and J. Nicolas\cite{Hausler05} and "A survey of motif discovery methods in an integrated framework" by Sandve and Drabløs\cite{SurveyMotifDiscovery}.

\section{Algorithmic techniques}

The algorithmic techniques can be largely classified into 1. pattern growth, 2. alignment-based and 3. probabilistic pattern discovery.

There are two basic ways to grow the patterns: iteration and combining. The iteration method uses a pattern and then starts iteratively expanding the pattern with new tokens. This approach can be very well optimized due to its simplicity, but it often requires more memory. The iterative approach can also have problems with larger patterns. The combining method first generates a set of simple patterns and then starts combining them to generate new patterns.

Alignment-based approaches work in two phases: 1. building a set of elementary patterns and 2. produce consensus pattern. The elementary patterns are usually very simple subsequences. The elementary patterns are aligned and merged to a consensus pattern that best describes all the patterns. The patterns could also be clustered before producing consensus patterns. This approach is usually done by two separate tools; one to generate frequent patterns and the other to align the patterns.

Probabilistic algorithms use a statistical model to iteratively improve the pattern parameters to identify the real patterns until a stop criteria is met. Common statistical techniques are Expectation Maximization (EM) and Gibbs sampling.

\subsection{Algorithms}

In this section we describe algorithms that we consider interesting or important in their algorithmic structure or approach. The list here is by no means exhaustive.

\paragraph{SPEXS \cite{spexs}} is an iterative pattern discovery algorithm. It iteratively grows a pattern trie while maintaining pattern occurrences of each query. Only patterns frequent enough are expanded. It can capture different regular expression tokens: groups (a token that matches multiple tokens in the alphabet) and wildcard positions (a token that matches any subsequence). It can also order the result based on interestingness criteria.

\paragraph{TEIRESIAS \cite{TEIRESIAS}} is a combining algorithm. It starts with a list of elementary patterns that occur at least $K$ times. Then it starts combining these elementary patterns into larger patterns. It uses the observation that a pattern P can be combined from pattern $A$, $B$ if the suffix of $A$ is the same as the prefix of $B$. For example, sequences \R{$\alpha\Delta$} and \R{$\Delta\beta$}can be combined into sequence \R{$\alpha\Delta\beta$}, where \R{$\alpha$}, \R{$\Delta$}, \R{$\beta$} are sequences.

\paragraph{MobyDick \cite{MobyDick}} is a combining algorithm. It starts with a dictionary of sequences and then looks for concatenated sequences $p$, which has a low P-value, based on the dictionary words, and then adds such $p$ to the dictionary.

\paragraph{SANSPOS \cite{NewDataStructures}} is a iterative approach that uses a positioning matrix to expand the pattern multiple tokens at a time. The use of positioning matrix can significantly increase the performance in the presence of short tandem repeats.

\section{Pattern rating}

There are many ways of comparing patterns to find the "most interesting" pattern. Of course, pairwise comparison is often wasteful and it is better to have an interestingness measure that we can calculate only by using the query. For practical purposes it is useful to represent that measure with a floating point value.

One useful property that an interestingness measure can have is monotonicity when patterns are ordered by length. This means that when we move from a small pattern to a larger pattern, the interestingness always either grows or decreases. The more common name for this idea is the \emph{Apriori principle}, which states that if a itemset is frequent, then all of its subsets must also be frequent, or if an itemset is infrequent, then all its supersets must also be infrequent. For example, if we make a pattern more specific, the number of matches can only decrease. This can be very helpful for pruning the search space. Mostly monotonic functions could provide probabilistic pruning, but we didn't find any information regarding it.

\begin{exmp}
Lets assume we are looking patterns that should have at least 10 matches. If we encounter a pattern \R{ACT} that has 8 matches then we do not have to examine patterns \R{xACT} and \R{ACTx}, where \R{x} is some token from the alphabet.
\end{exmp}

The most trivial measure for queries is the number of pattern matches in the dataset. When calculating the number of matches we must be aware that we can have multiple matches per sequence. For example, a pattern \R{AA} matches in sequence \R{AAAAAA} 5 times. It is better to count the number of sequences that contain the matches to account for such pathological cases.

Many patterns can occur by chance; therefore, it is important to remove such false positives from output is important. As previously mentioned, a frequent pattern is not necessarily interesting. We can use a reference dataset to compare the frequencies of patterns. There are different possibilities for a reference dataset: background sequence, shuffled input sequences, background Markov-Model, binomial/multinomial models. The background sequence is usually a similar dataset to the dataset we are analyzing; for example, if we select a subset from data for analyzing we can use the rest as a background sequence. If there are no background sequences we can simply shuffle the input sequences to get a "randomized" sample. To preserve more of the data characteristics we can build more complex models for randomization, such as hidden Markov Model\cite{RatingMarkovModel} or binomial and multinomial models.

We can specify interestingness measures using a reference dataset as a measurement for false positives. For example, one can use the ratio between the input sequence occurrence and background sequence occurrences. One problem with ratios is that, if the frequencies are small, then the ratios may be very high. By using binomial\cite{Binomial} or hypergeometric model we can estimate how probable the number of occurrences is in the input and the background dataset. There are also measures Z-score\cite{ZScores}, which estimates how many standard deviations an observation differs from the mean, and $\chi^2$-Value\cite{ChiValue}, which estimates whether a frequency distribution differs from theoretical distribution.

If we happen to have several queries with the same "score", then we can break the "tie" by comparing them with additional measures. One such useful measure is the complexity of a pattern. For example, if we have patterns \R{ATG} and \R{C} and both have the same number of occurrences then the longer pattern is probably more interesting.

\section{SPEXS}

SPEXS is a pattern discovery algorithm described in "Pattern Discovery from Biosequences"\cite{spexs}. It was designed to find frequently occuring patterns from sets of sequences. It constructs patterns by incrementally expanding the prefixes of the frequent patterns. It can generate several pattern classes: subsequences, subsequences containing group characters (a token where alternative characters from a list can be used), and patterns containing wildcard positions. The thesis describes several versions of the algorithm for finding specific pattern structures and also provides a general algorithm for pattern discovery. We only show some algorithms from the thesis.

The main idea of the general algorithm \ref{s:general} is that first we generate a pattern and a query that matches all possible positions in the sequence. We then put this query into a queue for extending. Extending a query means finding all queries whose patterns length is longer by 1. If any of the queries is fit, by some criteria, it will be put into the main queue, for further extension and to the output queue for possible output.

\begin{algorithm}
	\caption{The SPEXS algorithm}
	\label{s:general}
\begin{algorithmic}[1]
	\Require{String $S$, pattern class $\sym{P}$, output criteria, search order, and fitness measure $\sym{F}$}
	\Ensure{Patterns $\pi \in \sym{P}$ fulfilling all criteria, and output in the order of fitness $\sym{F}$}

	\State Convert input sequences into a single sequence
	\State Initiate data structures

	\Let{Root}{new node}
	\Let{Root.label}{$\epsilon$}
	\Let{Root.pos}{(1,2,...,n)}
	\State enqueue($\sym{Q}$, Root, order)

	\While{$N \gets$ dequeue($\sym{Q}$)}
		\State Create all possible extensions $p \in \sym{P}$ of $N$ using $N$.pos and $S$
		\For{ extension $p$ of $N$}
			\If{pattern $p$ and position list $p$.pos fulfill the criteria}
				\Let{$N$.child}{$p$}
				\State calculate $\sym{F}(p,S)$
				\State enqueue($\sym{Q}$,$p$,order)
				\If{$p$ fulfills the output criteria}
					\State store $p$ in output queue $\sym{O}$
				\EndIf
			\EndIf
		\EndFor
	\EndWhile
	\State{Report the list of top-ranking patterns from output queue $\sym{O}$}
\end{algorithmic}
\end{algorithm}

The simplest algorithm \ref{s:subsequences} is for finding subsequences from multiple sequences. The other algorithm \ref{s:groups} is for finding patterns group symbols from multiple sequences. These algorithms are performant, but they mix the idea of the algorithm and the optimizations of the algorithm, which means that we lose intuitiveness of the algorithms. The algorithms are here mainly for demonstrative purposes and we will not examine these algorithms, because there is a more intuitive approach explained in Chapter \ref{c:generalization}. 


\begin{algorithm}
	\caption{Generation of most "interesting" subsequences of a set of strings}
	\label{s:subsequences}
\begin{algorithmic}[1]
	\Require{Input strings $S^n = {S_1, ..., S_n}$, threshold $K$, interestingness $\sym{F}$}
	\Ensure{Subsequences that occur in at least K sequences of $S^n$ in the order of fitness $\sym{F}$}

	\Let{$S$}{$S_1 \# ... \# S_2$, $\# \not \in \Sigma$}
	\State Generate a mapping $\{1, 2, ..., |S|\} \mapsto \{1,2,...,n\}$ for countseq($Set$)
	\Let{Root}{new query}
	\Let{Root.label}{$\epsilon$}
	\Let{Root.pos}{$(1,2,...,|S|)$}
	\State enqueue($\sym{Q}$, Root)
	\While{$N \gets$ dequeue($\sym{Q}$)}
		\For{ $c \in \Sigma$ }
			\Let{$Set(c)$}{$0$}
		\EndFor
		\For{ $p \in N.pos$ }
			\State add $p+1$ to $Set(S[p])$ \textbf{unless} $p = |S|$ or $S[p] = \#$
		\EndFor
		\For{$c \in \Sigma$ \textbf{where} countseq($Set(c)$) $\geq K$}
			\Let{P}{new query}
			\Let{P.label}{$c$}
			\Let{P.pos}{$Set(c)$}
			\Let{N.child(c)}{$P$}
			\State enqueue($\sym{Q}$, P)
			\State enqueue($\sym{B}$, P, $\sym{F}$(P.pattern, $S^n$))
		\EndFor
		\State delete $N.pos$
	\EndWhile
	\While{$(N,f) \gets dequeue(\sym{B})$}
		\State output $N.pattern$ and $f$
	\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Generation of frequent patterns with character group positions}
	\label{s:groups}
\begin{algorithmic}[1]
	\Require{Input strings $S^n = {S_1, ..., S_n}$, threshold $K$, character groups $\Gamma$}
	\Ensure{Patterns from $\Sigma \cup \Gamma$ that occur in at least K sequences of$S^n$}

	\Let{$S$}{$S_1 \# ... \# S_2$, $\# \not \in \Sigma$}
	\State Generate a mapping $\{1, 2, ..., |S|\} \mapsto \{1,2,...,n\}$ for $countseq(Set)$
	\State Root $\gets$ new query; Root.label $\gets$ $\epsilon$
	\Let{Root.pos}{$(1,2,...,|S|)$}
	\State enqueue($\sym{Q}$, Root)
	\While{$N \gets$ dequeue($\sym{Q}$)}
		\State Output N.pattern
		\State // Construct the position list for pattern defined by node N
		\If{N.label $\in \Sigma$} 
			\Let{Pos}{N.pos}
		\Else 
			\Let{Pos}{$\bigcup_{c \in \text{N.label}}$ N.sibling($c$).pos}
		\EndIf
		\State // Group the positions according to characters in string S
		\For{ $c \in \Sigma$ }
			\Let{$Set(c)$}{$0$}
		\EndFor
		\For{ $p \in$ Pos }
			\State add $p+1$ to $Set(S[p])$ \textbf{unless} $p = |S|$ or $S[p] = \#$
		\EndFor
		\For{$c \in \Sigma$ \textbf{where} countseq($Set(c)$) $\geq K$}
			\Let{N.child($c$)}{ new query with label $c$}
			\Let{N.child($c$).pos}{$Set(c)$}
			\State enqueue($\sym{Q}$, N.child($c$))
		\EndFor
		\For{ $g \in \Gamma$ }
			\If{$\exists f (\Gamma \cup \Sigma, f \subset g, \sum_{c \in f} |Set(c)| = \sum_{c \in g} |Set(c)| $  }
				\State \textbf{continue}
			\EndIf
			\If{countseq($\cup_{c \in g} Set(c)$) $ \geq K$}
				\Let{N.child($g$)}{new query with label $g$}
				\For{$c \in g$}
					\Let{N.child(c)}{new query with label $c$ and positions $Set(S[c])$ }
				\EndFor
				\State enqueue($\sym{Q}$, N.child($g$))
			\EndIf
		\EndFor
		\If{all nodes N.sibling($c$), $c \in \Sigma \cup \Gamma$ have been expanded}
		\EndIf
	\EndWhile
\end{algorithmic}
\end{algorithm}
