\chapter{Approaches to Pattern Discovery}
\label{c:algorithms}

There have been suggested several ways of organizing patten discovery algorithms\cite{SurveyDNAMotif, SurveyMotifDiscovery, CombinatorialSubtle, Hausler05}. Here we give a overview of the different ideas; for thorough descriptions we suggest "Motif Discovery on Promotor Sequences" by M. Häußler and J. Nicolas\cite{Hausler05} and "A survey of motif discovery methods in an integrated framework" by Sandve and Drabløs\cite{SurveyMotifDiscovery}.

\section{Algorithmic techniques}

The algorithmic techniques can be largely classified into 1. pattern growth, 2. alignment based and 3. probabilistic pattern discovery.

There are to basic ways to grow the patterns: iteration and combining. The iteration method uses a pattern and then starts iteratively expanding the pattern with new tokens. This approach can be very fast for due to its simplicity, but often requires more memory. The iterative approach also can have problems with larger patterns. The combining method first generates a set of simple patterns and then starts combining them to generate new patterns.

Alignment based approaches work in two phases: 1. building a set of elementary patterns and 2. produce consensus pattern. The elementary patterns are usually very simple subsequences. The elementary patterns are aligned and merged to a consensus pattern that best describes all the patterns. The patterns could be also clustered before producing consensus patterns. This approach is usually done by two separate tools; one to generate frequent patterns and the other to align the patterns.

Probabilistic algorithms use a statistical model to iteratively improve the pattern parameters to identify the real patterns until a stop criteria is met. Common statistical techniques are Expectation Maximization (EM) and Gibbs sampling.

\subsection{Algorithms}

Here we describe algorithms that we consider interesting or important in their algorithmic structure or approach. The list here is by no means exhaustive.

\paragraph{SPEXS \cite{spexs}} is an iterative pattern discovery algorithm. It iteratively grows a pattern trie while maintaining pattern occurrences of each query. Only patterns frequent enough are expanded. It can capture different regular expression tokens: groups (a token that matches multiple tokens in the alphabet) and wildcard positions (a token that matches any subsequence). It can also order the result based on interestingness criteria.

\paragraph{TEIRESIAS \cite{TEIRESIAS}} is a combining algorithm. It starts with a list of elementary patterns that occur at least $K$ times. Then it starts combining these elementary patterns into larger patterns. It uses the observation that a pattern P can be combined from pattern $A$, $B$ if the suffix of $A$ is the same as the prefix of $B$. For example, sequences \R{$\alpha\Delta$} and \R{$\Delta\beta$}can be combined into sequence \R{$\alpha\Delta\beta$}, where \R{$\alpha$}, \R{$\Delta$}, \R{$\beta$} are sequences.

\paragraph{MobyDick \cite{MobyDick}} is a combining algorithm. It starts with a dictionary of sequences and then looks for concatenated sequences $p$, that has a low P-value, based on the dictionary words, and then adds such $p$ to the dictionary.

\paragraph{SANSPOS \cite{NewDataStructures}} is a iterative approach that uses a positioning matrix to expand the pattern multiple tokens at a time. The use of positioning matrix can significantly performance in the presence of short tandem repeats.

\subsubsection{SPEXS}

SPEXS is a pattern discovery algorithm described in "Pattern Discovery from Biosequences"\cite{spexs}. The thesis describes several versions of the algorithm for finding specific pattern structures, but it also provides a general algorithm.

\begin{algorithm}[H]
	\caption{The SPEXS algorithm}
\begin{algorithmic}[1]
	\Require{String $S$, pattern class $\sym{P}$, output criteria, search order, and fitness measure $\sym{F}$}
	\Ensure{Patterns $\pi \in \sym{P}$ fulfilling all criteria, and output in the order of fitness $\sym{F}$}

	\State Convert input sequences into a single sequence
	\State Initiate data structures

	\Let{Root}{new node}
	\Let{Root.label}{$\epsilon$}
	\Let{Root.pos}{(1,2,...,n)}
	\State enqueue($\sym{Q}$, Root, order)

	\While{$N \gets$ dequeue($\sym{Q}$)}
		\State Create all possible extensions $p \in \sym{P}$ of $N$ using $N$.pos and $S$
		\For{ extension $p$ of $N$}
			\If{pattern $p$ and position list $p$.pos fulfill the criteria}
				\Let{$N$.child}{$p$}
				\State calculate $\sym{F}(p,S)$
				\State enqueue($\sym{Q}$,$p$,order)
				\If{$p$ fulfills the output criteria}
					\State store $p$ in output queue $\sym{O}$
				\EndIf
			\EndIf
		\EndFor
	\EndWhile
	\State{Report the list of top-ranking patterns from output queue $\sym{O}$}
\end{algorithmic}
\end{algorithm}

The main idea of the algorithm is that first we generate a pattern and a query that matches all possible positions in the sequence. We then put this query into a queue for extending. Extending a query means finding all queries whose patterns length is longer by 1. If any of the queries is fit, by some criteria, it will be put into the main queue, for further extension, and output queue for possible output.

The algorithm is already very general, has many pattern types and provides different points for parallelization, hence it is good basis for designing a parallel pattern discovery algorithm.

\section{Pattern rating}

There are many ways of comparing patterns to find the "most interesting" pattern. Of course, pairwise comparison is often wasteful and it is better to have a interestingness measure that we can calculate only using the query. For practical purposes it is useful to represent that measure with a floating point value.

One useful property that an interestingness measure can have is monotonicity when patterns are ordered by length. This means that when we move from a small pattern to a larger pattern the interestingness always either grows or decreases. The more common name for this idea is \emph{Apriori principle}, which states that if a itemset is frequent, then all of its subsets must also be frequent or if an itemset s infrequent then all its supersets must also be infrequent. For example, if we make a pattern more specific the number of matches can only decrease. This can be very helpful for pruning the search space. Mostly monotonic functions could provide probabilistic pruning, but we didn't find any information about it.

\begin{exmp}
Lets assume we are looking patterns that should have at least 10 matches. If we encounter a pattern \R{ACT} that has 8 matches then we do not have to examine patterns \R{xACT} and \R{ACTx}, where \R{x} is some token from the alphabet.
\end{exmp}

The most trivial measure for patterns is the number of pattern matches in the dataset. When calculating number of matches we must be aware that we can have multiple matches per sequence. For example, a pattern \R{AA} matches in sequence \R{AAAAAA} 6 times. It is better to count the number of sequences that contain the matches to account for such pathological cases.

Many patterns can occur by chance and removing such false positives from output is important. As previously mentioned a frequent pattern is not necessarily interesting. We can use a reference dataset to compare the frequencies of patterns. There are different possibilities for a reference dataset: background sequence, shuffled input sequences, background Markov-Model, binomial/multinomial models. The background sequence is usually a similar dataset to the dataset we are analyzing; for example, if we select a subset from data for analyzing we can use the rest as a background sequence. If there are no background sequences we can simply shuffle the input sequences to get a "randomized" sample. To preserve more of the data characteristic we can build more complex models for randomization such as hidden Markov Model\cite{RatingMarkovModel} or binomial and multinomial models.

We can specify interestingness measures using a reference dataset as a measurement for false positives. One trivial way is to use the ratio between the input sequence occurrence and background sequence occurrences. One problem with ratios is that, if the frequencies are small then the ratios may be very high. Using binomial\cite{Binomial} or hypergeometric model we can estimate how probable is the number of occurrences in input and background dataset. There are also measures Z-score\cite{ZScores}, which estimates how many standard deviations an observation differs from the mean, and $\chi^2$-Value\cite{ChiValue}, which estimates whether a frequency distribution differs from theoretical distribution.

If we are ordering several patterns with the same "score" then we can break the "tie" by comparing more measures. One such measure is the complexity of a pattern. For example, when we have patterns \R{AATTGGG} and \R{C} and both have the same number of occurrences then the more specific pattern is probably more interesting.
